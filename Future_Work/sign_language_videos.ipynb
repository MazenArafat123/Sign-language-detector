{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPsfDGfMRVz+9dhc24Ibcm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samyadel123/sign-language-project-/blob/main/sign_language.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "upload you kaggle api"
      ],
      "metadata": {
        "id": "X1Sr_l1fX46V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "ql-3KKKmS0Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "4EkJenO6UWTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "downloding the dataset"
      ],
      "metadata": {
        "id": "fr21YXZkX-jn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download risangbaskoro/wlasl-processed"
      ],
      "metadata": {
        "id": "EUnVk5ooWbrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/wlasl-processed.zip"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OCpWwAR6WjP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loud_json(file_name:str):\n",
        "  import json\n",
        "  with open(file_name,\"r\") as f:\n",
        "    data = json.load(f)\n",
        "  return data"
      ],
      "metadata": {
        "id": "l6Ld1wQVRq3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "code to see the vido"
      ],
      "metadata": {
        "id": "n9jYgf8vdVdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import time\n",
        "# Import the Colab-specific display function and utility for clearing output\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Set the path to your video file\n",
        "video_path = \"/content/videos/00335.mp4\"\n",
        "\n",
        "# Open the video file\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "if not cap.isOpened():\n",
        "    print(f\"Error: Could not open video file at {video_path}\")\n",
        "else:\n",
        "    # Get the original FPS for a smoother display (optional but recommended)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    if fps > 0:\n",
        "        delay = 1 / fps\n",
        "    else:\n",
        "        delay = 0.033 # Default to ~30 FPS if not found\n",
        "\n",
        "    # Loop through the video frames\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            # If the frame was not read successfully, break the loop\n",
        "            break\n",
        "\n",
        "        # 1. Display the frame using the Colab-specific function\n",
        "        cv2_imshow(frame)\n",
        "\n",
        "        # 2. Clear the previous output cell to simulate video\n",
        "        # The 'wait=True' ensures a smooth transition\n",
        "        #clear_output(wait=True)\n",
        "\n",
        "        # Optional: Add a small sleep to regulate frame rate (improves playback quality)\n",
        "        time.sleep(delay)\n",
        "\n",
        "    # Release the VideoCapture object\n",
        "    cap.release()\n",
        "    print(\"Video playback finished.\")\n",
        "\n",
        "# Note: cv2.destroyAllWindows() is NOT needed in Colab."
      ],
      "metadata": {
        "id": "vYtYRE7kXuMd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AssemblyAI-Examples/mediapipe-python.git\n",
        "!pip install mediapipe\n",
        "!pip install PyQt5\n",
        "!pip install ipython==7.32.0"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Ek1BT2ApgkAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapipe as mp\n",
        "def mediapip_dedection(fram,model):\n",
        "  fram = cv2.cvtColor(fram,cv2.COLOR_BGR2RGB) # color conversion for mediapipe\n",
        "  fram.flags.writeable = False\n",
        "  results = model.process(fram)\n",
        "  fram.flags.writeable = True\n",
        "  fram = cv2.cvtColor(fram,cv2.COLOR_RGB2BGR)\n",
        "  return fram,results\n",
        "\n",
        "mp_holistic = mp.solutions.holistic\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n"
      ],
      "metadata": {
        "id": "lpZVLJ_4dYrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_landmarks(image, results):\n",
        "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
        "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
        "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
        "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION)\n",
        "    return image"
      ],
      "metadata": {
        "id": "MvJmAm6lnYwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_stayled_landmarks(image,results):\n",
        "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
        "                              mp_drawing.DrawingSpec(color=(80,22,10),thickness=1,circle_radius=1),\n",
        "                              mp_drawing.DrawingSpec(color=(80,44,121),thickness=1,circle_radius=1)\n",
        "                              )\n",
        "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
        "                              mp_drawing.DrawingSpec(color=(121,22,76),thickness=1,circle_radius=1),\n",
        "                              mp_drawing.DrawingSpec(color=(121,44,250),thickness=1,circle_radius=1)\n",
        "                              )\n",
        "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
        "                              mp_drawing.DrawingSpec(color=(245,117,66),thickness=1,circle_radius=1),\n",
        "                              mp_drawing.DrawingSpec(color=(245,66,230),thickness=1,circle_radius=1)\n",
        "                              )\n",
        "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
        "                              mp_drawing.DrawingSpec(color=(80,110,10),thickness=1,circle_radius=1),\n",
        "                              mp_drawing.DrawingSpec(color=(80,256,121),thickness=1,circle_radius=1)\n",
        "                            )\n",
        "\n",
        "    return image"
      ],
      "metadata": {
        "id": "BuQZnqnsqrNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
        "  cap = cv2.VideoCapture(\"/content/videos/00335.mp4\")\n",
        "  while cap.isOpened():\n",
        "    ret,frame = cap.read()\n",
        "    if not ret:\n",
        "      break\n",
        "    # make dedection\n",
        "    image, result = mediapip_dedection(frame,holistic)\n",
        "\n",
        "    # draw landmarks\n",
        "    image = draw_stayled_landmarks(image,result)\n",
        "    cv2_imshow(image)\n",
        "    #\n",
        "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "      break\n",
        "\n",
        "  cap.release()\n",
        "  cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "xiHKqt1MkbzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now i know that the result landmark has four things\n",
        "- pos landmark\n",
        "- left-hand landmark\n",
        "- right-hand landmark\n",
        "- face landmarks\n",
        "\n",
        "for example pos landmark is a list for each fram in the vedio\n",
        "\n",
        "and each one has four componats (x,y,z,visabilty)"
      ],
      "metadata": {
        "id": "23rnMSHbr1aO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result.pose_landmarks.landmark[0]"
      ],
      "metadata": {
        "id": "CXUUyjyFr0uU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.face_landmarks.landmark[0]"
      ],
      "metadata": {
        "id": "zRfXveajuQNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "extract medipipe points in one fram using this function"
      ],
      "metadata": {
        "id": "aDe0Sqt_6NpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def extract_keypoints(results):\n",
        "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
        "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
        "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
        "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
        "    return np.concatenate([pose, face, lh, rh])\n"
      ],
      "metadata": {
        "id": "4UdHQwdI5q0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def extract_frame_sequence(video_path, start_frame, end_frame, output_prefix=\"frame\",output_path='/content'):\n",
        "    \"\"\"\n",
        "    Extracts a sequence of frames from a video using OpenCV.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): Path to the video file.\n",
        "        start_frame (int): The index of the starting frame (inclusive).\n",
        "        end_frame (int): The index of the ending frame (inclusive).\n",
        "        output_prefix (str): Prefix for the saved frame filenames.\n",
        "    \"\"\"\n",
        "    # 1. Open the video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video file {video_path}\")\n",
        "        return\n",
        "\n",
        "    if end_frame == -1:\n",
        "      end_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n",
        "\n",
        "    # Check if the end frame is before the start frame\n",
        "    if end_frame < start_frame:\n",
        "        print(\"Error: End frame cannot be less than the start frame.\")\n",
        "        cap.release()\n",
        "        return\n",
        "\n",
        "    # Get the total number of frames in the video\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    print(f\"Total frames in video: {total_frames}\")\n",
        "\n",
        "    # Check if frame numbers are within bounds\n",
        "    if end_frame >= total_frames:\n",
        "        print(f\"Warning: End frame {end_frame} is beyond the total frames. Setting end_frame to {total_frames - 1}\")\n",
        "        end_frame = total_frames - 1\n",
        "\n",
        "    # 2. Set the Start Frame\n",
        "    # cv2.CAP_PROP_POS_FRAMES is the 0-based index of the frame to be decoded/captured next.\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
        "    current_frame_index = start_frame\n",
        "    frame_count = 0\n",
        "\n",
        "    print(f\"Starting extraction from frame {start_frame} to {end_frame}...\")\n",
        "    with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
        "      # 3. Loop and Read\n",
        "      while cap.isOpened() and current_frame_index <= end_frame:\n",
        "          # Read the next frame\n",
        "          ret, frame = cap.read()\n",
        "\n",
        "          if ret:\n",
        "              # 4. Save/Process the frame\n",
        "              # Example: Save the frame as a PNG file\n",
        "              # filename = f\"{output_prefix}_{current_frame_index:05d}.png\"\n",
        "              # filename = os.path.join(base_path,filename)\n",
        "              filename = os.path.join(output_path,f\"{output_prefix}_{current_frame_index:05d}.npy\")\n",
        "              # medipipe dedection\n",
        "              image, results = mediapip_dedection(frame,holistic)\n",
        "              # extract keypoints\n",
        "              keypoints = extract_keypoints(results)\n",
        "\n",
        "              #cv2.imwrite(filename, frame)\n",
        "              np.save(filename,keypoints)\n",
        "              # Increment the frame count\n",
        "              frame_count += 1\n",
        "              current_frame_index += 1\n",
        "          else:\n",
        "              # Break the loop if we can't read the next frame\n",
        "              print(f\"Warning: Could not read frame at index {current_frame_index}. Stopping.\")\n",
        "              break\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()\n",
        "    print(f\"Extraction complete. {frame_count} frames saved.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "IhSU5vc-8LUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's make an experiment"
      ],
      "metadata": {
        "id": "jUDkrOOv8mG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "load the config file and get a test exampe to work with"
      ],
      "metadata": {
        "id": "991O7zJO8sYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## function to exract imges"
      ],
      "metadata": {
        "id": "a8wst4CKgMo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "data = loud_json(\"/content/WLASL_v0.3.json\")\n",
        "one = data[0]\n",
        "# lable / gloss\n",
        "gloss = one[\"gloss\"]\n",
        "# instances\n",
        "instances = one[\"instances\"]\n",
        "\n",
        "# get the first vido in the first sample\n",
        "video_id = instances[0][\"video_id\"]\n",
        "frame_start = instances[0][\"frame_start\"]\n",
        "frame_end = instances[0][\"frame_end\"]\n",
        "\n",
        "# constract vido path\n",
        "video_path = os.path.join(\"/content/videos\",f\"{video_id}.mp4\")\n",
        "\n",
        "output_prefix = gloss\n",
        "\n"
      ],
      "metadata": {
        "id": "fEkHej8r8qgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "extract the frams i need"
      ],
      "metadata": {
        "id": "sbrWawGe8yo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# see a sample frame\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "sample_imge = \"/content/book_00049.png\"\n",
        "# display image\n",
        "img = cv2.imread(sample_imge)\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ezvodPw38jTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## setting folder structure"
      ],
      "metadata": {
        "id": "I-BJCZzGgHQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now i want to make a dict with the key as the gloss and a list of vido id's for each gloss\n",
        "import json\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class unit:\n",
        "  video_id:str\n",
        "  frame_start:int\n",
        "  frame_end:int\n",
        "\n",
        "with open(\"/content/WLASL_v0.3.json\",\"r\") as f:\n",
        "  data = json.load(f)\n",
        "gloss_vido_dict = {}\n",
        "for sample in data:\n",
        "  gloss = sample[\"gloss\"]\n",
        "  instances = sample[\"instances\"]\n",
        "  for instance in instances:\n",
        "    video_id = instance[\"video_id\"]\n",
        "    frame_start = instance[\"frame_start\"]\n",
        "    frame_end = instance[\"frame_end\"]\n",
        "    unit_instance = unit(\n",
        "        video_id=video_id,\n",
        "        frame_start=frame_start,\n",
        "        frame_end=frame_end\n",
        "    )\n",
        "    if gloss not in gloss_vido_dict:\n",
        "      gloss_vido_dict[gloss] = []\n",
        "    gloss_vido_dict[gloss].append(unit_instance)"
      ],
      "metadata": {
        "id": "fKOLkQs-8kKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# folder strucre\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "base_exraction_path = \"/content/MyData\"\n",
        "# make my data folder\n",
        "os.makedirs(base_exraction_path,exist_ok=True)\n",
        "\n",
        "# loop over gloss\n",
        "for gloss in gloss_vido_dict:\n",
        "  # loop over vido id\n",
        "  for unit in gloss_vido_dict[gloss]:\n",
        "    # make a folder with (/content/mydat/gloss(class name)/vido_id/our exracted frams)\n",
        "    # make the path to vedio\n",
        "    video_folder_path = os.path.join(base_exraction_path, gloss, unit.video_id)\n",
        "    # make the folder\n",
        "    os.makedirs(video_folder_path,exist_ok=True)\n",
        "    os.makedirs(video_folder_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "ELqy_JFe8j7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now let's do the exraction\n",
        "for gloss in gloss_vido_dict:\n",
        "  # loop over vido id\n",
        "  for unit in gloss_vido_dict[gloss]:\n",
        "    start_frame = unit.frame_start\n",
        "    end_frame = unit.frame_end\n",
        "    video_path = os.path.join(\"/content/videos\",f\"{unit.video_id}.mp4\")\n",
        "    output_prefix = os.path.join(base_exraction_path,gloss,unit.video_id)\n",
        "    output_path = os.path.join(base_exraction_path,gloss,unit.video_id)\n",
        "    extract_frame_sequence(video_path,start_frame,end_frame,output_prefix,output_path=output_path)"
      ],
      "metadata": {
        "id": "6WWyHVaf8knk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the exracted foldet to mydrive to work on it directly\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!zip -r /content/MyData.zip /content/MyData\n",
        "!cp /content/MyData.zip /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "Vu3lUQpi8k0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # extract land marks form resuls\n",
        "out=extract_keypoints(results=result)\n",
        "np.save(\"/content/test.npy\",out)"
      ],
      "metadata": {
        "id": "wtXJTW_I8lA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3snjafk_8lLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OenBgdm48lU2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}